{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Architecture](#architecture)\n",
    "    - [Activation Funcions](#activation-functions)\n",
    "    - [Backpropagation](#backpropagation)\n",
    "- [Multiclass Classification](#multiclass-classification)\n",
    "    - [One Hot Encoding](#one-hot-encoding)\n",
    "    - [Problem Definition](#problem-definition)\n",
    "    - [Softmax Layer](#softmax-layer)\n",
    "- [Machine Learning](#machine-learning)\n",
    "    - [Linear Model](#linear-model)\n",
    "    - [Feedforward Neural Network](#feedforward-neural-network)\n",
    "- [Challenges](#challenges)\n",
    "    - [MNIST](#mnist)\n",
    "    - [Boston](#boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture  <a class=\"anchor\" id=\"architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src='../assets/NeuralNetwork.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    f(\\mathbf{X}) = \\boldsymbol{\\sigma(\\mathbf{X}*\\mathbf{w_{1}} + \\mathbf{b_{1}})*\\mathbf{w_{2}} + \\mathbf{b_{2}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden **Affine Layer**: with parameters $\\mathbf{w_{1}}, \\mathbf{b_{1}}$\n",
    "and activation function $\\boldsymbol{\\sigma}$ contributes to the feature extraction.\n",
    "    - automation of **Feature Engineering**\n",
    "* Ouput **Linear Layer**: parameters $\\mathbf{w_{2}}, \\mathbf{b_{2}}$\n",
    "performs Linear Regression on the \"complex\" features, extracted from the network \n",
    "    - flexibility of **Non-Linear Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions <a class=\"anchor\" id=\"activation-functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma$ is the activation function of the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Function\n",
    "\n",
    "The Neural Network maps $\\mathbf{X}$ to $\\mathbf{y}$ according to:\n",
    "\\begin{align}\n",
    "    f(\\mathbf{X}) &= \\mathbf{\\hat{y}}\\\\\n",
    "    f(\\mathbf{X}) &= \\boldsymbol{\\sigma(\\mathbf{X}*\\mathbf{w_{1}} + \\mathbf{b_{1}})*\\mathbf{w_{2}} + \\mathbf{b_{2}}}  \n",
    "\\end{align}\n",
    "\n",
    "For $\\boldsymbol{\\sigma(X)} = \\mathbf{X}$:\n",
    "\\begin{align}\n",
    "    f(\\mathbf{X}) &= (\\mathbf{X}*\\mathbf{w_{1}} + \\mathbf{b_{1})*\\mathbf{w_{2}} + \\mathbf{b_{2}}}\\\\\n",
    "    f(\\mathbf{X}) &= \\mathbf{X}*\\mathbf{w_{1}}*\\mathbf{w_{2}} + \\mathbf{b_{1}}*\\mathbf{w_{2}}\\\\\n",
    "    f(\\mathbf{X}) &= \\mathbf{X}*\\mathbf{\\tilde{w}} + \\mathbf{\\tilde{b}}\n",
    "\\end{align}\n",
    "\n",
    "> Consequently, the model has the same capacity with a simple **Linear Model**.\n",
    "\n",
    "This applies to all the hidden layers, but not the output,\n",
    "which can have an identity activation function for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation <a class=\"anchor\" id=\"backpropagation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the **Stochastic Gradient Descent (SGD)** optimisation algorithm, the update rules for the parameters:\n",
    "\n",
    "$$\\mathbf{w^{(t+1)}} = \\mathbf{w^{(t)}} + \\eta \\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{w}}$$\n",
    "and\n",
    "$$\\mathbf{b^{(t+1)}} = \\mathbf{b^{(t)}} + \\eta \\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{b}}$$\n",
    "where:\n",
    "* $\\mathcal{J}$: the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification  <a class=\"anchor\" id=\"multiclass-classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition <a class=\"anchor\" id=\"problem-definition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**: task of choosing a class, category for a given input.\n",
    "\n",
    "**Binary Classification**: the input belongs to any of **two** classes.\n",
    "\n",
    "**Multi-class Classification**: there is a finite number of classes, greater than two, for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding  <a class=\"anchor\" id=\"one-hot-encoding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an $M$-class Classification, convert the scalar output to an $M$-dimensional vector\n",
    "with a single **1** at the $i_{th}$ component if the datapoint belongs to the $i_{th}$ class\n",
    "and all other components to **0**.\n",
    "\n",
    "For example, for a **3-class Classification** `{1: red, 2: blue, 3: yellow}` problem:\n",
    "* `1 = [1 0 0]` $\\rightarrow$ `red` class\n",
    "\n",
    "* `2 = [0 1 0]` $\\rightarrow$ `blue` class\n",
    "\n",
    "* `3 = [0 0 1]` $\\rightarrow$ `yellow` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def int_to_onehot(x, num_classes):\n",
    "    \"\"\"Convert an `int` to one hot (binary) format.\"\"\"\n",
    "    tmp = np.zeros(num_classes, dtype=int)\n",
    "    tmp[-x - 1] = 1\n",
    "    return tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer <a class=\"anchor\" id=\"softmax-layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an **$M$-class Classification** using **One Hot Encoding** the output layer is $M$-dimensional.\n",
    "In order to quantify predictability of the network and provide confidence intervals of selected class,\n",
    "output layer values are turned to probabilities using **Softmax Function**:\n",
    "\n",
    "$$p_{j}(x) = \\frac{e^{x_{j}}}{\\sum_{m=1}^{M}e^{x_{m}}}, \\quad j=1, 2, ..., M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax Layer - Forward Pass\"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning  <a class=\"anchor\" id=\"machine-learning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scientific computing library\n",
    "import numpy as np\n",
    "\n",
    "# visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# show plots without need of calling `.show()`\n",
    "%matplotlib inline\n",
    "\n",
    "# prettify plots\n",
    "plt.rcParams['figure.figsize'] = [16.0, 12.0]\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper module for toy data generation\n",
    "from adapter import make_spiral, array_to_onehot\n",
    "# helper module for data visualization\n",
    "import visualize\n",
    "\n",
    "# number of samples per class\n",
    "N = 100\n",
    "# dimensionality of input space\n",
    "D = 2\n",
    "# number of classes to classify\n",
    "K = 3\n",
    "\n",
    "# generate toy data\n",
    "X, _y = make_spiral(N, D, K, one_hot=False)\n",
    "# convert class label to one_hot vector\n",
    "y = array_to_onehot(_y, K)\n",
    "\n",
    "# plot data to 2D plane\n",
    "plt.scatter(X[:, 0], X[:, 1], c=_y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.title('Spiral Toy Data\\n')\n",
    "plt.xlabel('$x_{1}$')\n",
    "plt.ylabel('$x_{2}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model  <a class=\"anchor\" id=\"linear-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Linear Classifier\n",
    "\n",
    "# Model Parameters\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 1e-0 # learning rate\n",
    "n_epochs = 150 # number of epochs\n",
    "\n",
    "# Number of datapoints\n",
    "N = X.shape[0]\n",
    "\n",
    "# Softmax layer\n",
    "softmax = lambda s: s / np.sum(s, axis=1, keepdims=True)\n",
    "\n",
    "# Store history of loss\n",
    "hist = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "  \n",
    "    # Feedforward pass: class scores, [N x K]\n",
    "    scores = np.dot(X, W) + b\n",
    "\n",
    "    # Convert scores to probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    # Softmax Layer\n",
    "    probs = softmax(exp_scores)\n",
    "\n",
    "    # Cross-Entropy Error\n",
    "    corect_logprobs = -np.log(probs[range(N),_y])\n",
    "    loss = np.sum(corect_logprobs) / N\n",
    "    \n",
    "    # Log\n",
    "    hist.append(loss)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    dscores = probs\n",
    "    dscores[range(N), _y] = dscores[range(N), _y] - 1\n",
    "    dscores = dscores / N\n",
    "\n",
    "    # Compute gradients\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    # Update gradients\n",
    "    W = W - (eta * dW)\n",
    "    b = b - (eta * db)\n",
    "\n",
    "# Plot training error\n",
    "plt.figure()\n",
    "plt.plot(hist)\n",
    "plt.title('Learning Curve\\n')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Cross-Entropy Error')\n",
    "plt.legend(['Training Error']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "scores = np.dot(X, W) + b\n",
    "prediction = np.argmax(scores, axis=1)\n",
    "\n",
    "'training accuracy: %.3f' % (np.mean(prediction == _y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize separation lines\n",
    "visualize.linear(X, _y, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network  <a class=\"anchor\" id=\"feedforward-neural-network\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Step | Question                             | Answer                         |\n",
    "| ---- |--------------------------------------| ------------------------------ |\n",
    "| 1    | Labeled Data?                        | $\\textbf{X}$ and $\\textbf{y}$  |\n",
    "| 2    | Number of features of $\\mathbf{X}$   | 2                              |\n",
    "| 3    | Number of components of $\\mathbf{y}$ | 3 (one-hot-encoding)           |\n",
    "| 4    | Size of hidden layer                 | ???                            |\n",
    "| 5    | Problem Nature                       | **Multi-class Classification** |\n",
    "| 6    | Loss Function                        | **Cross Entropy Error**        |\n",
    "| 7    | Update Rules                         | **Backpropagation Algorithm**  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Single Hidden Layer Feedforward Neural Network\n",
    "\n",
    "# Hyperparameters\n",
    "h = 100 # size of hidden layer\n",
    "eta = 1e-0 # learning rate\n",
    "n_epochs = 3000 # number of epochs\n",
    "\n",
    "# D: number of features\n",
    "# h: size of hidden layer\n",
    "# K: number of output classes\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "\n",
    "# Number of datapoints\n",
    "N = X.shape[0]\n",
    "\n",
    "# ReLU activation function\n",
    "\n",
    "\n",
    "# Softmax layer\n",
    "\n",
    "\n",
    "# Store history of loss\n",
    "hist = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "  \n",
    "    # Feedforward pass: class scores, [N x K]\n",
    "\n",
    "\n",
    "    # Convert scores to probabilities\n",
    "    \n",
    "    # Softmax Layer\n",
    "    \n",
    "\n",
    "    # Cross-Entropy Error\n",
    "    \n",
    "    \n",
    "    # Log\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "  \n",
    "    # Backward pass: compute gradients\n",
    "\n",
    "    # Compute gradients\n",
    "    # Backprop: W2, b2\n",
    "    \n",
    "    # Backprop: hidden layer\n",
    "    \n",
    "    # Backprop: ReLU\n",
    "    \n",
    "    # Backprop: W1, b1\n",
    "    \n",
    "\n",
    "    # Update rule: Stochastic Gradient Descent\n",
    "    \n",
    "\n",
    "# Plot training error\n",
    "plt.figure()\n",
    "plt.plot(hist)\n",
    "plt.title('Learning Curve\\n')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Cross-Entropy Error')\n",
    "plt.legend(['Training Error']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "hidden_layer = relu(np.dot(X, W1) + b1)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "'training accuracy: %.3f' % (np.mean(predicted_class == _y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize separation curves\n",
    "visualize.nn(X, _y, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tensorflow Playground](http://playground.tensorflow.org/)\n",
    "\n",
    "Binary classification game, built with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges  <a class=\"anchor\" id=\"challenges\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston  <a class=\"anchor\" id=\"boston\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression for `Boston` dataset using:\n",
    "* a Linear Model\n",
    "* a Single Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load Data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Dimensions\n",
    "N, D = X.shape\n",
    "\n",
    "# make sure input-output sizes are consistent\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST  <a class=\"anchor\" id=\"mnist\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-class classification** of handwritten digits for `MNIST` dataset using:\n",
    "* a Linear Model\n",
    "* a Single Layer Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Load Data\n",
    "mnist = fetch_mldata('MNIST original', data_home='.')\n",
    "X, y = mnist['data'], mnist['target'].astype(int)\n",
    "\n",
    "# Dimensions\n",
    "N, D = X.shape\n",
    "\n",
    "# make sure input-output sizes are consistent\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 4 random images from dataset\n",
    "_indeces = np.random.randint(0, N, 4)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(221+i)\n",
    "    # visualize, 28x28 pixels\n",
    "    ax.imshow(X[_indeces[i]].reshape(28, 28))\n",
    "    # print true class\n",
    "    ax.set_title('True class: %s' % y[_indeces[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
